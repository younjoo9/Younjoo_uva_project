{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Group Assignment 1 (a&b)\n",
    "## Enzo Keuning (12878502), Hsi Yun Chien (12534919), Younjoo Mo (12475440)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (a):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0: rent out; 1: maintanence\n",
    "action = range(2)\n",
    "states = (1, 2, 3, 4) \n",
    "num_of_states = len(states)\n",
    "negative_infinity = float('-inf')\n",
    "rewards = np.array([[60, negative_infinity], [60, -40], [60, -40], [negative_infinity, -100]])\n",
    "\n",
    "\n",
    "#define state transition probability\n",
    "p = np.array([[[0.8, 0.1, 0.1, 0], [0, 0, 0, 0]], \n",
    "              [[0, 0.6, 0.3, 0.1], [1, 0, 0, 0]],\n",
    "              [[0, 0, 0.5, 0.5], [1, 0, 0, 0]], \n",
    "              [[0, 0, 0, 0],[1, 0, 0, 0]]])\n",
    "\n",
    "\n",
    "v       = [0, 0, 0, -100]\n",
    "\n",
    "\n",
    "\n",
    "pi      = np.zeros(num_of_states, dtype=int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  60.   50.   10. -100.]     Optimal policy for day 8: [0 0 0 1]\n",
      "[114.  83.  20. -40.]     Optimal policy for day 7: [0 0 1 1]\n",
      "[161.5 111.8  74.   14. ]     Optimal policy for day 6: [0 0 1 1]\n",
      "[207.8 150.7 121.5  61.5]     Optimal policy for day 5: [0 0 1 1]\n",
      "[253.5 193.  167.8 107.8]     Optimal policy for day 4: [0 0 1 1]\n",
      "[298.9 236.9 213.5 153.5]     Optimal policy for day 3: [0 0 1 1]\n",
      "[344.2 281.5 258.9 198.9]     Optimal policy for day 2: [0 0 1 1]\n",
      "[389.4 326.5 304.2 244.2]     Optimal policy for day 1: [0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "r = []\n",
    "for i in range(8):\n",
    "#iterate 8 times\n",
    "    k = []\n",
    "    for s in range(0,4):\n",
    "        if i == 0:\n",
    "            vold = v[s]\n",
    "        else:\n",
    "            vold = r[i-1][s]\n",
    "        vmax = np.zeros(4)\n",
    "        if i == 0:\n",
    "            q = rewards[s] + np.dot(p[s],v)\n",
    "        else:\n",
    "            q = rewards[s] + np.dot(p[s],r[i-1])\n",
    "        k.append(max(q))  \n",
    "        vmax[s]  = max(vmax[s], abs(vold-k[s]))\n",
    "        pi[s] = np.argmax(q)\n",
    "    r.append(k)\n",
    "    r[i] = np.round(r[i],1)\n",
    "    print(f\"{r[i]}     Optimal policy for day {8-i}: {pi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (b):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy    :  [0 0 1 1]\n",
      "Estimated v-values:  [473.90182244 421.1830066  386.5116402  326.5116402 ]\n"
     ]
    }
   ],
   "source": [
    "p = np.array([[[0.8, 0.1, 0.1, 0], [0, 0, 0, 0]],\n",
    "              [[0, 0.6, 0.3, 0.1], [1, 0, 0, 0]],\n",
    "              [[0, 0, 0.5, 0.5], [1, 0, 0, 0]], \n",
    "              [[0, 0, 0, 0], [1, 0, 0, 0]]])\n",
    "rewards_b = np.array([ [60, negative_infinity], [60, -40], [60, -40], [negative_infinity, -100]])\n",
    "\n",
    "v_b       = np.zeros(4)\n",
    "pi_b      = np.zeros(4, dtype=np.int)\n",
    "states_b  = (1, 2, 3, 4)\n",
    "actions = range(2)\n",
    "gamma   = 0.9\n",
    "\n",
    "for s in range(0,4):\n",
    "    v_b[s]  = max(rewards_b[s,])\n",
    "    pi_b[s] = np.argmax(rewards_b[s,])\n",
    "    \n",
    "vmax_b = 999\n",
    "while vmax_b > 1.0e-5:\n",
    "    vmax_b = 0\n",
    "    for s in range(0,4):\n",
    "        vold_b  = v_b[s]\n",
    "        q_b     = rewards_b[s] + gamma*np.dot(p[s],v_b)\n",
    "        v_b[s]  = max(q_b)\n",
    "        vmax_b  = max(vmax_b, abs(vold_b-v_b[s]))\n",
    "        pi_b[s] = np.argmax(q_b)        \n",
    "        #print(v, pi)\n",
    "    #print(vmax)\n",
    "\n",
    "print(\"Optimal policy    : \", pi_b)\n",
    "print(\"Estimated v-values: \", v_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
