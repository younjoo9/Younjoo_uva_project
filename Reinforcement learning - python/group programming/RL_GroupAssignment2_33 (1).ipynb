{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning 2021-22: Second Group Assignment\n",
    "\n",
    "**Deadline**: Mon 14 March 2022, 23:59\n",
    "\n",
    "|Nr|**Name**|**Student ID**|**Email**|\n",
    "|--|--------|--------------|---------|\n",
    "|1.|Enzo Keuning        | 12878502             | gje.keuning@gmail.com        |\n",
    "|2.|Hsi Yun Chien        | 12534919             | hsiyunchien@gmail.com        |\n",
    "|3.| Jamie Mo        | 12475440             |    younjoo737@gmail.com     |\n",
    "\n",
    "**Declaration of Originality**\n",
    "\n",
    "We whose names are given under 1., 2., and 3. above declare that:\n",
    "\n",
    "1. These solutions are solely our own work.\n",
    "2. We have not made (part of) these solutions available to any other student.\n",
    "3. We shall not engage in any other activities that will dishonestly improve my results or dishonestly improve or hurt the results of others.\n",
    "\n",
    "## Instructions for completing and submitting the assignment\n",
    "Please pay attention to the following instructions:\n",
    "\n",
    "1. Please follow carefully the steps outlined in the assignment. If you cannot solve an exercise and this hinders continuing with subsequent exercises, try to find a way to work around it and give a clear explanation for the solution you have chosen.\n",
    "2. Submit your work in the form of a Jupyter notebook and a html file (see point 6 below) via Canvas, before the deadline. Your notebook should not give errors when executed with `Run All`.\n",
    "3. Most of your answers will consist of code. Make sure your code is well structured, efficient and provided with comments where needed. These aspects will be taken into account in the grading of your work.\n",
    "4. Sometimes you are asked to answer some (open) questions. Please type your answer in the given Markdown cells and use your own words in answering those questions.\n",
    "5. You are allowed to work on the assignment in groups of no more than 3 students and to submit together.\n",
    "6. Save the complete work with all outputs in an HTML file: \"File\"->\"Download as\" -> \"HTML (.html)\" in Jupyter Notebook. Upload this HTML file to canvas as a supplement. You must submit BOTH notebook and HTML files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi Problem\n",
    "\n",
    "In this assignment, you will solve the taxi problem for the following map:\n",
    "\n",
    "````\n",
    "        +---------+\n",
    "        |R: | : :G|\n",
    "        | : | : : |\n",
    "        | : : : : |\n",
    "        | | : | : |\n",
    "        |Y| : |B: |\n",
    "        +---------+\n",
    "````\n",
    "\n",
    "There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends. The rewards are:\n",
    "\n",
    "- -1 per step unless other reward is triggered.\n",
    "- +20 delivering passenger.\n",
    "- -10 executing \"pickup\" and \"drop-off\" actions illegally.\n",
    "\n",
    "If a navigation action would cause the taxi to hit a wall (solid borders in the map), the action is a no-op, and there is only the usual reward of −1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "tqdm_disabled_episode=False\n",
    "\n",
    "GroupNumber=33 # Input your group number Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the ``TaxiEnv`` class from env_taxi.py and create an instance of the Taxi environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_taxi import TaxiEnv\n",
    "env=TaxiEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we reset the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 2, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns four state variables:\n",
    "\n",
    "- Row index of the taxi (starting from 0 in Python)\n",
    "- Colum index of the taxi (starting from 0 in Python)\n",
    "- Passenger location (0-4): 0=R, 1=G, 2=Y, 3=B, 4=in taxi\n",
    "- Destination location (0-3): same encoding rule as that for passenger location but excluding the \"in taxi\" option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the ``describe_state`` method of the Taxi instance to display the state variables without location encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Taxi Location': [2, 3], 'Passenger Location': 'Y', 'Destination Index': 'G'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.describe_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the ``render`` method to visualize the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 6 discrete deterministic actions:\n",
    "- 'South': move south\n",
    "- 'North': move north\n",
    "- 'East': move east\n",
    "- 'West': move west\n",
    "- 'Pickup': pickup passenger\n",
    "- 'Dropoff': drop off passenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['South', 'North', 'East', 'West', 'Pickup', 'Dropoff']\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us move one step to west:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 2, 2, 1), -1, False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step('West')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a 3-tuple: the new state (a list of 4 numbers), reward, and whether the episode is ended. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the new state. Note that the last action is shown at the bottom as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1** Complete the codes below to generate _one_ episode for the taxi driver who:\n",
    "- Picks up the passenger when the taxi is at the location of the passenger when they are not yet at the destination;\n",
    "- Drops off the passenger in the taxi when reaching the destination;\n",
    "- Moves randomly with equal probabilities when finding the passenger or when the passenger is in the taxi (but not yet at destination). \n",
    "\n",
    "Then print the sum of rewards for the taxi driver in your episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "{'Taxi Location': [2, 3], 'Passenger Location': 'Y', 'Destination Index': 'B'}\n"
     ]
    }
   ],
   "source": [
    "state=env.reset(271) # The number 271 fixes the initial state. Do not change it.\n",
    "env.render()\n",
    "print(env.describe_state())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us program the policy function for the taxi driver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_policy(state):\n",
    "    ## Start Coding Here\n",
    "    dest = env.locs[state[3]]\n",
    "    taxi = state[:2]\n",
    "    if state[2] == 4: #passenger is in taxi\n",
    "        if dest == taxi: #if the taxi is at the destination then 'dropoff'\n",
    "            action = env.action_space[5]\n",
    "        else: \n",
    "            action = np.random.choice(env.action_space[0:4]) #otherwise take a random action except 'dropoff'\n",
    "    else:\n",
    "        if env.locs[state[2]] == dest: #if the passenger is at the destination, then the taxi takes a random action except it cannot 'pickup' or 'dropoff'\n",
    "            action = np.random.choice(env.action_space[0:4]) \n",
    "        elif taxi == env.locs[state[2]]: #if the taxi is at the passenger location, then 'pickup'\n",
    "            action = env.action_space[4]\n",
    "        else: \n",
    "            action = np.random.choice(env.action_space[0:4]) #else the taxi takes a random action\n",
    "    ## End Coding Here                                      \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function simulate_episode in module env_taxi:\n",
      "\n",
      "simulate_episode(env, policy, max_iter=inf)\n",
      "    Simulate a episode following a given policy\n",
      "    @param env: game environment\n",
      "    @param policy: policy (a dictionary or a function)\n",
      "    @return: resulting states, actions and rewards for the entire episode\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the simulate_episode function from env_taxi.py to help you simulate episode(s)\n",
    "from env_taxi import simulate_episode\n",
    "help(simulate_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return for this episode: -10\n"
     ]
    }
   ],
   "source": [
    "# Set the seed to make results reproducible\n",
    "np.random.seed(GroupNumber)\n",
    "\n",
    "states, actions, rewards = simulate_episode(env,naive_policy)\n",
    "\n",
    "# Save the sum of rewards in this variable G\n",
    "G=np.sum(rewards)\n",
    "\n",
    "\n",
    "# Print the return for this episode\n",
    "print(\"Return for this episode:\", G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2** Following the steps below to investigate whether we can solve the optimal policy by using Monte Carlo Exploring Starts. If possible, complete the codes and save the optimal (greedy) policy as a dictionary/defaultdict object. Otherwise, comment on the issues you encounter. \n",
    "\n",
    "Follow the instructions below to complete the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function simulate_episode_ES in module env_taxi:\n",
      "\n",
      "simulate_episode_ES(env, policy, max_iter=inf)\n",
      "    Simulate a episode with exploring starts\n",
      "    @param env: game environment\n",
      "    @param policy: policy (a dictionary or a function)\n",
      "    @param max_iter: maximum number of time steps\n",
      "    @return: resulting states, actions and rewards for the entire episode\n",
      "\n",
      "[(1, 1, 0, 3), (2, 1, 0, 3), (1, 1, 0, 3), (1, 1, 0, 3), (1, 1, 0, 3), (0, 1, 0, 3), (0, 1, 0, 3), (0, 1, 0, 3), (0, 0, 0, 3), (0, 0, 4, 3), (0, 1, 4, 3), (1, 1, 4, 3), (1, 0, 4, 3), (1, 0, 4, 3), (1, 1, 4, 3), (1, 1, 4, 3), (1, 0, 4, 3), (1, 1, 4, 3), (2, 1, 4, 3), (3, 1, 4, 3), (3, 2, 4, 3), (3, 2, 4, 3), (4, 2, 4, 3), (3, 2, 4, 3), (2, 2, 4, 3), (2, 1, 4, 3), (2, 2, 4, 3), (2, 3, 4, 3), (3, 3, 4, 3), (3, 3, 4, 3), (4, 3, 4, 3), (4, 3, 3, 3)] ['South', 'North', 'East', 'East', 'North', 'North', 'East', 'West', 'Pickup', 'East', 'South', 'West', 'West', 'East', 'East', 'West', 'East', 'South', 'South', 'East', 'East', 'South', 'North', 'North', 'West', 'East', 'East', 'South', 'West', 'South', 'Dropoff'] [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 20]\n"
     ]
    }
   ],
   "source": [
    "# We import the simulate_episode_ES function from env_taxi.py to simulate episodes with exploring starts\n",
    "from env_taxi import simulate_episode_ES\n",
    "help(simulate_episode_ES)\n",
    "\n",
    "\n",
    "# Let us try to simulate one episode with exploring start for the naive policy from Exercise 1\n",
    "# Print the states, actions and rewards for an episode (the first 200 steps only)\n",
    "np.random.seed(GroupNumber)\n",
    "states, actions, rewards=simulate_episode_ES(env, naive_policy, 200)\n",
    "print(states, actions, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can solve the optimal policy by using Monte Carlo Exploring Starts, write down your codes in the next cell and save the optimal (greedy) policy in the dictionary object ``pi_ES`` initialized below. Use no more than 50000 episodes. We will import the ``performance_evaluation`` function from env_taxi.py to evaluate your policy in new episodes (that terminate within 1000 time steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your greedy actions as the values of this dictionary, whose keys are the states\n",
    "\n",
    "#pi= defaultdict(lambda: np.random.choice(env.action_space))\n",
    "#np.random.seed(GroupNumber)\n",
    "\n",
    "# ## Start Coding Here \n",
    "\n",
    "#def mc_control_on_policy(env,gamma=1, n_episode=500): \n",
    "#    n_action = len(env.action_space)\n",
    "#    G_sum = defaultdict(float)\n",
    "#    N = defaultdict(int)\n",
    "    \n",
    "    # Initialization\n",
    "    \n",
    "#    Q = defaultdict(lambda: dict(zip(['South', 'North', 'East', 'West', 'Pickup', 'Dropoff'], np.empty(6))))\n",
    "    \n",
    "#    for episode in tqdm(range(n_episode)):\n",
    "#        states_t, actions_t, rewards_t = simulate_episode(env)\n",
    "        \n",
    "        # Calculate the returns for all state-action pairs in this episode\n",
    "        \n",
    "#        return_t = 0\n",
    "#        G = {}\n",
    "#        for state_t, action_t, reward_t in zip(states_t[::-1][1:],rewards_t[::-1]):# remove the last one \n",
    "#            return_t = gamma * return_t + reward_t\n",
    "#            G[(state_t, action_t)] = return_t\n",
    "\n",
    "        # Update the action-value estimate and policy\n",
    "    \n",
    "#        for state_action, return_t in G.items():\n",
    "#            state, action = state_action\n",
    "#            if state[2] != state[3]:\n",
    "#                G_sum[state_action] += return_t\n",
    "#                N[state_action] += 1\n",
    "#                Q[state][action] = G_sum[state_action] / N[state_action]\n",
    "#                pi[state]=max(Q[state],key=Q[state].get)\n",
    "#    return Q, pi\n",
    "\n",
    "## End Coding Here\n",
    " \n",
    "\n",
    "\n",
    "# # We import the performance_evaluation function from env_taxi.py to evaluate your optimal policy\n",
    "#from env_taxi import performance_evaluation\n",
    "#performance_evaluation(env,pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you cannot solve the optimal policy by using Monte Carlo Exploring Starts, explain the issue(s) and why they cannot be addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Monte Carlo Exploring Starts is looping forever for each episode, choosing states randomly such that all the pairs of state and action have a probability >0, however in this question, there are some actions that are not allowed to go from certain states. Like for example, the driver couldnt \"Pickup\" if he hasn't arrived in location of passenger. Therefore, we could not solve the optimal policy using monte carlo exploring starts. \n",
    "(We still give the potential code, but inserted hashtags so it is not evaluated)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**:  Find the optimal policy by using Q-learning with 50000 episodes and the exploration probability $\\varepsilon=0.1$. Try two different values of the step-size parameter $\\alpha=0.4$ and $\\alpha=0.1$. Compare their performance in 10000 new episodes and comment on the similarities or/and differences. \n",
    "\n",
    "Follow the instructions below to complete the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [01:06<00:00, 755.77it/s]\n",
      "100%|██████████| 50000/50000 [01:06<00:00, 756.76it/s]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(GroupNumber)\n",
    "\n",
    "## Start Coding Here: Find the optimal policy by using Q-learning with 50000 episodes\n",
    "def gen_epsilon_greedy_policy(action_space, Q, epsilon):\n",
    "    \"\"\"\n",
    "    Generate epsilon greedy policy as a function\n",
    "    @param action_space: list of all actions\n",
    "    @param Q: the action values, Q[state] is a dictitonary\n",
    "    @param epsilon: value of epsilon\n",
    "    @return: epsilon-greedy policy as a function\n",
    "    \"\"\"\n",
    "    def epsilon_greedy_policy(state):\n",
    "        greedy_action=max(Q[state],key=Q[state].get)\n",
    "        random_action=np.random.choice(action_space)\n",
    "        return np.random.choice([random_action,greedy_action],p=[epsilon,1-epsilon])\n",
    "    \n",
    "    return epsilon_greedy_policy\n",
    "\n",
    "def q_learning(env, gamma, n_episode, alpha, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Off-policy Q-learning\n",
    "    @param env: game environment\n",
    "    @param gamma: discount factor\n",
    "    @param n_episode: number of episodes\n",
    "    @param alpha: step-size parameter\n",
    "    @param epsilon: exploring probability\n",
    "    @return: the optimal Q-function, the optimal policy, length of episode and the sum of rewards per episode\n",
    "    \"\"\"\n",
    "    length_episode = np.zeros(n_episode) \n",
    "    sum_reward_episode = np.zeros(n_episode)\n",
    "    \n",
    "    Q = defaultdict(lambda: dict(zip(env.action_space, np.zeros(len(env.action_space)))))\n",
    "    epsilon_greedy_policy = gen_epsilon_greedy_policy(env.action_space, Q, epsilon)\n",
    "    \n",
    "    pi= defaultdict(lambda: np.random.choice(env.action_space))\n",
    "    \n",
    "    for episode in tqdm(range(n_episode), disable=tqdm_disabled_episode):\n",
    "        ## Start Coding Here\n",
    "        \n",
    "        state = env.reset()\n",
    "        is_done = False\n",
    "        # Generate behavior using epsilon greedy policy\n",
    "        action = epsilon_greedy_policy(state)\n",
    "        while not is_done:\n",
    "            next_state, reward, is_done = env.step(action)\n",
    "            next_action = epsilon_greedy_policy(next_state)\n",
    "            \n",
    "            # A special case of off-policy Expected Sarsa with epsilon=0\n",
    "            v= max(Q[next_state].values())\n",
    "            \n",
    "            td_delta = reward + gamma * v - Q[state][action]\n",
    "            \n",
    "            Q[state][action] += alpha * td_delta\n",
    "            # Updte Target Policy\n",
    "            pi[state]=max(Q[state],key=Q[state].get)\n",
    "            \n",
    "            length_episode[episode] += 1\n",
    "            sum_reward_episode[episode] += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            \n",
    "        ## End Coding Here    \n",
    "    return Q, pi, length_episode, sum_reward_episode\n",
    "\n",
    "# Save the policy for alpha=0.4 as a Python dictionary/defaultdic object pi_qlearning_1\n",
    "Q_qlearning_1, pi_qlearning_1, length_episode_qlearning_1, sum_reward_episode_qlearning_1 = q_learning(env, 1, 50000, 0.4, 0.1)\n",
    "\n",
    "# Save the policy for alpha=0.1 as a Python dictionary/defaultdic object pi_qlearning_2\n",
    "Q_qlearning_2, pi_qlearning_2, length_episode_qlearning_2, sum_reward_episode_qlearning_2 = q_learning(env, 1, 50000, 0.1, 0.1)\n",
    "\n",
    "\n",
    "## End Coding Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 399/10000 [00:00<00:04, 2017.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------  Q-learning, alpha=0.4  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2100.56it/s]\n",
      "  4%|▍         | 419/10000 [00:00<00:04, 2107.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of rewards per episode: 7.8718\n",
      "The percentage of episodes that cannot terminate within 1000 steps: 0.0\n",
      "--------  Q-learning, alpha=0.1  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2101.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of rewards per episode: 7.8886\n",
      "The percentage of episodes that cannot terminate within 1000 steps: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare their performance in 10000 new episodes: Do you find a big difference?\n",
    "from env_taxi import performance_evaluation\n",
    "\n",
    "print('--------  Q-learning, alpha=0.4  --------')\n",
    "performance_evaluation(env,pi_qlearning_1)\n",
    "\n",
    "print('--------  Q-learning, alpha=0.1  --------')\n",
    "performance_evaluation(env,pi_qlearning_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commment on the similarities or/and differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: We see that both the Q-learning method with a learning rate of $\\alpha=0.1$ and $\\alpha=0.4$ always terminate within 1000 steps, so the taxi always managed to dropoff the passenger at destination within 1000 iterations. However the sum of the rewards per episode are slightly higher in the method with the smaller learning rate ($\\alpha=0.1$), this implies that on average it took slightly less time/iterations, or/and with less mistakes, for the taxi to bring the passenger to the right destination with the Q-learning method that has a learning rate of $\\alpha=0.1$ compared to the method with a learning rate of $\\alpha=0.4$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**: Follow the steps below to find the optimal $\\varepsilon$-soft policy by using Sarsa with 50000 episodes and the exploration probability $\\varepsilon=0.1$. Try two different values of the step-size parameter $\\alpha=0.4$ and $\\alpha=0.1$. Compare their performance, and comment on the similarties or/and differences with that of Q-learning.\n",
    "\n",
    "Follow the instructions below to complete the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [01:06<00:00, 754.68it/s]\n",
      "100%|██████████| 50000/50000 [01:02<00:00, 796.13it/s]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(GroupNumber)\n",
    "\n",
    "## Start Coding Here\n",
    "def gen_epsilon_greedy_policy(action_space, Q, epsilon):\n",
    "    def epsilon_greedy_policy(state):\n",
    "        greedy_action=max(Q[state],key=Q[state].get)\n",
    "        random_action=np.random.choice(action_space)\n",
    "        return np.random.choice([random_action,greedy_action],p=[epsilon,1-epsilon])\n",
    "    \n",
    "    return epsilon_greedy_policy\n",
    "\n",
    "\n",
    "# Save the policy for alpha=0.4 in a Python function pi_sarsa_1\n",
    "# Save the policy for alpha=0.1 in a Python function pi_sarsa_2\n",
    "\n",
    "\n",
    "def sarsa(env, gamma, n_episode, alpha, epsilon):\n",
    "    \"\"\"\n",
    "    Obtain the optimal policy with on-policy SARSA algorithm\n",
    "    @param env: game environment\n",
    "    @param gamma: discount factor\n",
    "    @param n_episode: number of episodes\n",
    "    @param alpha: step-size parameter\n",
    "    @param epsilon: exploring probability\n",
    "    @@return: the optimal Q-function, the optimal epsilon-greedy policy (as a function), length of episode and the sum of rewards per episode\n",
    "    \"\"\"\n",
    "    \n",
    "    Q = defaultdict(lambda: dict(zip(env.action_space, np.zeros(len(env.action_space)))))\n",
    "    \n",
    "    pi = gen_epsilon_greedy_policy(env.action_space, Q, epsilon)\n",
    "    \n",
    "    for _ in tqdm(range(n_episode), disable=tqdm_disabled_episode):\n",
    "        \n",
    "        state = env.reset()\n",
    "        is_done = False\n",
    "        action = pi(state)\n",
    "        while not is_done: \n",
    "            #getting the next state\n",
    "            next_state, reward, is_done = env.step(action)\n",
    "            \n",
    "            #choosing the next action using the target policy (epsilon-greedy here)\n",
    "            next_action = pi(next_state)\n",
    "            \n",
    "            td_delta = reward + gamma * Q[next_state][next_action] - Q[state][action]\n",
    "            #Learning the Q-value, decomposed formula\n",
    "            Q[state][action] += alpha * td_delta\n",
    "            \n",
    "            if is_done:\n",
    "                break\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            \n",
    "    return Q, pi\n",
    "\n",
    "# End Coding Here\n",
    "Q_sarsa_1, pi_sarsa_1 = sarsa(env, 1, 50000, 0.4, 0.1)\n",
    "Q_sarsa_2, pi_sarsa_2 = sarsa(env, 1, 50000, 0.1, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 62/10000 [00:00<00:38, 255.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------  Sarsa, alpha=0.4  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:33<00:00, 301.16it/s]\n",
      "  2%|▏         | 165/10000 [00:00<00:11, 843.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of rewards per episode: -37.44326383319968\n",
      "The percentage of episodes that cannot terminate within 1000 steps: 0.0024\n",
      "--------  Sarsa, alpha=0.1  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:12<00:00, 811.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of rewards per episode: -0.883\n",
      "The percentage of episodes that cannot terminate within 1000 steps: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from env_taxi import performance_evaluation\n",
    "\n",
    "print('--------  Sarsa, alpha=0.4  --------')\n",
    "performance_evaluation(env,pi_sarsa_1)\n",
    "\n",
    "print('--------  Sarsa, alpha=0.1  --------')\n",
    "performance_evaluation(env,pi_sarsa_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare their performance, and comment on the similarties or/and differences with that of Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: We observe that SARSA with the larger learning rate, $\\alpha = 0.4$, a small fraction of the episodes does not terminate, which implies that in some episodes that taxi is not able to dropoff the passenger within 1000 steps. This is not the case when a smaller learning rate is chosen, $\\alpha = 0.1$, then all episodes terminate within a 1000 steps. From theory, we know that a smaller learning rate will take more time to converge, and a higher learning rate adjusts more abruptly, which might lead to non-convergence of the algorithm. \n",
    "Decreasing the learning rate, $\\alpha$, from 0.4 to 0.1 has a tremendous effect on the results of the SARSA algorithm. When a learning rate of 0.4 is used the sum of the rewards per episode it approximately -37.44, whereas with a smaller learning rate of 0.1 the sum of the rewards per episode is a negative number with a dramatically smaller absolute value. Which implies that a smaller learning rate works remarkably better in the SARSA algorithm. Intuitively, it makes sense that alpha influences SARSA to a greater extent as SARSA is an on-policy algorithm and Q-learning an off-policy algorithm.\n",
    "\n",
    "In contrast, in Q-learning we observed a positive sum of rewards per episode for both values for $\\alpha$, with only a small-scale positive change when taking the smaller stepsize, $\\alpha = 0.1$. Q-learning is an off-policy algorithm that is more agressive, in the sense that it is willing to take more risk to have a quick iterating algorithm. Whereas the SARSA algorithm will take less risk, resulting in a longer, but safer iteration. \n",
    "\n",
    "In the taxi-problem mistakes are relatively not that costsly (for example in comparison with the Cliff Waling problem form the book), hence it makes intuitive sense that Q-learning gives a higher sum of rewards per episode. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**: Do you expect double Q-learning will improve Q-learning substantially? Motivate your answer. You do not need to implement the double Q-learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "\n",
    "Yes. All the algorithms we implement for control problems involve maximizing the target policy. For example, the target policy of Q learning is greedy policy ,whereas the target policy of SARSA is epsilon-greedy policy. The fact that these algorithms try to max over all the actions ( and use the maximum value of the estimated action-value as the maximum value of the actual action-value) overestimate the value function estimates and action-value estimates, which is known as the maximization bias problem. \n",
    "\n",
    "To avoid causing such large bias, double Q-learning divides the sample, one for generating independent estimates and their actual values, the other one for determining the optimal action. This approach is then unbiased. \n",
    "\n",
    "However, in the taxi-problem we have that all rewards are non-stochastic, hence we do not expect a substantial overestimation of the value function estimates and action-value estimates. Therefore, we expect that double Q-learning will not improve Q-learning substantially. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6** \n",
    "Follow the steps below to investigate whether we can solve the optimal policy by using off-policy Monte Carlo control with weighted importance sampling. Use the optimal $\\varepsilon$-soft policy found by Sarsa above with $\\alpha=0.1$ as the behavior policy and use no more than 50000 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(GroupNumber)\n",
    "\n",
    "## Start Coding Here\n",
    "\n",
    "# Save your optimal policy as a dictionary/defaultdict Python object pi_wis\n",
    "def mc_control_off_policy_weighted(env, n_episode, behaviour_policy, Q_sarsa, gamma = 1):\n",
    "\n",
    "    C = defaultdict(float) # initial value=0\n",
    "    \n",
    "    # Initialization\n",
    "    Q = defaultdict(lambda: dict(zip(env.action_space, np.empty(len(env.action_space)))))\n",
    "    pi= defaultdict(lambda: np.random.choice(env.action_space))\n",
    "    \n",
    "    for _ in tqdm(range(n_episode)):\n",
    "        states_t, actions_t, rewards_t = simulate_episode(env, behaviour_policy)\n",
    "        \n",
    "        W = 1.\n",
    "        return_t = 0.\n",
    "        for state_t, action_t, reward_t in zip(states_t[::-1][1:], actions_t[::-1], rewards_t[::-1]):\n",
    "            return_t = gamma * return_t + reward_t\n",
    "            C[(state_t, action_t)] += W\n",
    "            Q[state_t][action_t] += (W / C[(state_t, action_t)]) * (return_t - Q[state_t][action_t])\n",
    "            pi[state_t]=max(Q[state_t],key=Q[state_t].get)\n",
    "            \n",
    "            if action_t != pi[state_t]:\n",
    "                break\n",
    "            elif action_t == max(Q_sarsa[state_t], key=Q_sarsa[state_t].get):\n",
    "                b = (1-0.1)+0.1*(1/len(env.action_space))\n",
    "            else:\n",
    "                b = 0.1*(1/len(env.action_space))\n",
    "            W *= 1./ b\n",
    "                \n",
    "    return Q, pi\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## End Coding Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the total number of distinct states visited in your episodes. Have you reached all 400 possible states (within the episode)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [01:03<00:00, 788.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of States Visited: 347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Q_wis, pi_wis = mc_control_off_policy_weighted(env, 50000, pi_sarsa_2, Q_sarsa_2)\n",
    "\n",
    "print('Number of States Visited:', len(pi_wis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a complete policy that:\n",
    "- follows your optimal policy (in this exercise) if the state is visited in your episodes (in this exercise);\n",
    "- otherwise follows the naive policy in Exercise 1.\n",
    "\n",
    "If you have reached all possible states in your episodes, it is identical to your optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_policy(state):\n",
    "    ## Start Coding Here\n",
    "    if state in Q_wis:\n",
    "        action = pi_wis[state]\n",
    "    else:\n",
    "        action = naive_policy(state)    \n",
    "    ## End Coding Here\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:29<00:00, 343.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of rewards per episode: 7.636850152905199\n",
      "The percentage of episodes that cannot terminate within 1000 steps: 0.0844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "performance_evaluation(env,complete_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7**: What are your conclusions from the analysis above? What are the advantages and disadvantages of these methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: We can see that the optimal policy by using off-policy Monte Carlo control with weighted importance sampling\n",
    "with a stepsize of $\\alpha=0.1$ only reached 347 states. The sum of the rewards per episodes in the complete policy, including off-policy MC and the initial naive policy, was only marginally smaller than of Q-learning. This could be interpreted as casusing slightly more time for the taxi to bring the passenger to the right destination, or making a little more mistakes along the way compared to Q-learning. But, also note that the complete ploicy from q6 has a considerable higher percentage of episodes that cannot terminate within a 1000 iterations.  \n",
    "\n",
    "The sum of rewards per episode was substantially smaller and negative in the optimal $\\varepsilon$-soft policy found by Sarsa. \n",
    "Hence, it seems like the Q-learning method gives the most optimal policy in the sense that it gives the highest rewards per episode and it always terminates within 1000 steps. \n",
    "\n",
    "As earlier mentioned, the Q-learning method is riskier as it has a higher variance, but it may also converge quicker. In contrast, the Sarsa method will take less risk and may converge slower, but safer. The MC off-policy method does not visit all states and it can only update when the return at the end of an episode is known. \n",
    "In the taxi-problem mistakes are relatively incostly, hence the fast-iterating Q-learning method gives the best results. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
